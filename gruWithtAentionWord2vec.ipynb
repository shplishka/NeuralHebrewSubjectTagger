{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#!pip install tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import gensim\n",
    "from scipy import spatial\n",
    "\n",
    "orginal_sentences_oath_file = 'Data/Hebrew/SVLM_Hebrew_Wikipedia_Corpus.txt'\n",
    "tagged_sentences_path_file = 'Data/Hebrew/parsed.txt'\n",
    "csv_hebrew_sentences_path_file = 'Data/Hebrew/Hebrew_tagged_sentences.csv'\n",
    "NUM_EXAMPLES = 50000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract subject for each tagged sentence\n",
    "def extract_sbj(sentences):\n",
    "    listoflists = []\n",
    "    sublist = []\n",
    "    for i in sentences:\n",
    "        if i.find(\"SBJ\") !=-1:\n",
    "            i = re.sub(r\"[^א-ת\\\"]+\", \" \", i)\n",
    "            if len(i)>3:\n",
    "                sublist.append(i)\n",
    "        elif i == '\\n':\n",
    "            listoflists.append(sublist)\n",
    "            sublist = []\n",
    "    return listoflists\n",
    "\n",
    "#flat the list of subject to one string\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s))\n",
    "\n",
    "#match subject to orginal sentence\n",
    "def make_pairs(orginal_sentences,tagged_sentences,num_examples):\n",
    "    subjects = extract_sbj(tagged_sentences)\n",
    "    clear_lines = []\n",
    "    for tag, sentence in zip(subjects[:num_examples],orginal_sentences[:num_examples]):\n",
    "        clear_lines.append(listToString(tag) + '\\t' + sentence)\n",
    "    subject = []\n",
    "    sentences = []\n",
    "    for line in clear_lines:\n",
    "        line = line.split('\\t')\n",
    "        sentences.append(line[1])\n",
    "        subject.append(line[0])\n",
    "    return subject, sentences\n",
    "\n",
    "def make_csv_subject_sentence(orginal_sentences,tagged_sentences, csv_path,num_examples):\n",
    "    subjects, sentences = make_pairs(orginal_sentences,tagged_sentences,num_examples)\n",
    "    trainDF = pd.DataFrame()\n",
    "    trainDF['subject'] = subjects[:num_examples]\n",
    "    trainDF['sentence'] = sentences[:num_examples]\n",
    "    mask = (trainDF['subject'].str.len()>1)\n",
    "    trainDF = trainDF.loc[mask]\n",
    "    trainDF.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_sentences = open(orginal_sentences_oath_file).readlines()\n",
    "tagged_sentences = open(tagged_sentences_path_file).readlines()\n",
    "make_csv_subject_sentence(orginal_sentences,tagged_sentences,csv_hebrew_sentences_path_file,NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_hebrew_sentences_path_file,index_col=[0])\n",
    "NUM_EXAMPLES = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence_Hebrew(w):\n",
    "    #w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    #w = re.sub(r\"([?.!,¿\\\"])\", r\" \\1 \", w)\n",
    "    #w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^א-ת\\\"]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> מבחינתי אפשר להעלות אותו לרשימת ההמתנה לשמוע הערות ואח\"כ להצבעה <end>\n"
     ]
    }
   ],
   "source": [
    "example_sentence = preprocess_sentence_Hebrew(\"מבחינתי אפשר להעלות אותו לרשימת ההמתנה לשמוע הערות ואח\\\"כ להצבעה\")\n",
    "print(example_sentence)\n",
    "example_sentence = [subject for subject in example_sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_csv(csv_path, num_examples):\n",
    "    df = pd.read_csv(csv_path,index_col=[0])\n",
    "    subjects = [preprocess_sentence_Hebrew(subject) for subject in df['subject']]  \n",
    "    sentences = [preprocess_sentence_Hebrew(sentence) for sentence in df['sentence']] \n",
    "    subjects = [subject.split() for subject in subjects]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "    return subjects, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, sentences = create_dataset_from_csv(csv_hebrew_sentences_path_file, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, subjects_train, subjects_test =\\\n",
    "train_test_split(sentences, subjects, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word_model = gensim.models.Word2Vec(subjects+sentences, size=EMBEDDING_DIM, min_count=1, window=5, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word, word_model):\n",
    "    if word in word_model.wv.vocab:\n",
    "        return word_model.wv.vocab[word].index\n",
    "    else:\n",
    "        return 1 #default index for non-exsits in vec_model voacb\n",
    "    \n",
    "def idx2word(idx, word_model):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence, word_model):\n",
    "  for w in sentence:\n",
    "      print (\"%d ----> %s\" % (word2idx(w,word_model),w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes(sentence,word_model,max_length_inp):\n",
    "    data_set = [word2idx(w,word_model) for w in sentence]\n",
    "    data_set = tf.keras.preprocessing.sequence.pad_sequences([data_set],max_length_inp, padding='post', value=1)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create matrix of the dataset indexes \n",
    "def create_dataset_word2vec_matrix(sentences,vec_model,max_length_inp):\n",
    "    data_set = []\n",
    "    #creates train data set\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        data_set.append(list())\n",
    "        data_set[i] = ([word2idx(word,vec_model) for word in sentence])\n",
    "    data_set = tf.keras.preprocessing.sequence.pad_sequences(data_set,max_length_inp, padding='post', value=1)\n",
    "    return data_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len, max_subjects_len  = max_length(sentences_train), max_length(subjects_train)\n",
    "x_train = create_dataset_word2vec_matrix(sentences_train,word_model,max_sentence_len)\n",
    "y_train = create_dataset_word2vec_matrix(subjects_train,word_model,max_subjects_len)\n",
    "x_test = sentences_test\n",
    "y_test = subjects_test\n",
    "max_length_inp, max_length_targ  = x_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ----> <start>\n",
      "233 ----> מבחינתי\n",
      "79 ----> אפשר\n",
      "821 ----> להעלות\n",
      "48 ----> אותו\n",
      "4239 ----> לרשימת\n",
      "10519 ----> ההמתנה\n",
      "1130 ----> לשמוע\n",
      "403 ----> הערות\n",
      "14250 ----> ואח\"כ\n",
      "1619 ----> להצבעה\n",
      "1 ----> <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,   233,    79,   821,    48,  4239, 10519,  1130,   403,\n",
       "        14250,  1619,     1,     1,     1]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(example_sentence,word_model)\n",
    "sentence_to_indexes(example_sentence,word_model,max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  import sys\n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "units = 1024\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size = word_model.wv.syn0.shape[0]\n",
    "emdedding_size = word_model.wv.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(y_train))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, _ = next(iter(dataset))\n",
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim,pretrained_weights, maxlen, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[pretrained_weights],input_length=maxlen,trainable=True)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, emdedding_size, pretrained_weights, max_length_inp ,units, BATCH_SIZE)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, pretrained_weights, maxlen, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[pretrained_weights],input_length=maxlen,trainable=True)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "decoder = Decoder(vocab_size, emdedding_size, pretrained_weights, max_length_targ ,units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([word_model.wv.vocab['<start>'].index] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    inputs = sentence_to_indexes(sentence,word_model, max_length_inp)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    hidden  = tf.zeros((1, units))\n",
    "    print(inputs.shape)\n",
    "\n",
    "    enc_out, enc_hidden = encoder(inputs,hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "\n",
    "    dec_input = tf.expand_dims([word_model.wv.vocab['<start>'].index], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions,dec_hidden = decoder(dec_input,dec_hidden,enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "#         attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "#         attention_plot[t] = attention_weights.numpy()\n",
    "        # print(\"evaluate attention_plot[\",t,\"]\",attention_plot[t])\n",
    "\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if word_model.wv.index2word[predicted_id] != '<end>':\n",
    "            result += word_model.wv.index2word[predicted_id] + ' '\n",
    "\n",
    "        if word_model.wv.index2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KDtree_nearest(test_sentence,result):\n",
    "    A = [word_model[word] for word in test_sentence.split()]\n",
    "    tree = spatial.KDTree(A)\n",
    "    word_result = result.split()\n",
    "    a = word_model[word_result[0]]\n",
    "    index = tree.query(a)[1]\n",
    "    return test_sentence.split()[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    result = KDtree_nearest(sentence,result)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted subject: {}'.format(result))\n",
    "    sentence = sentence[::-1]\n",
    "    result = result[::-1]\n",
    "    sentence_splited = sentence.split(' ')\n",
    "    #to fix the hebrew changes\n",
    "    sentence_splited.reverse()\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence_splited, result.split(' '))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14)\n",
      "Input: הוא לא יודע מה עובר עליי\n",
      "Predicted subject: עליי\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sentence = \"הוא לא יודע מה עובר עליי\"\n",
    "result = tag(test_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
