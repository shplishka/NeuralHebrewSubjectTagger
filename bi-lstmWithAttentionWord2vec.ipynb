{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject neural tagger seq2seq with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a sequence to sequence (seq2seq) model for finding the subject (SBJ) in a sentence. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Hebrew sentence, such as *\"באנציקלופדיה אנחנו משתדלים לדווח כמה שפחות על דברים שקרו לאחרונה\"*, and return the subject words: *\"אנחנו\"*\n",
    "\n",
    "The tagging quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while tagging:\n",
    "\n",
    "for the sentence:\n",
    "```\"הוא לא יודע מה עובר עליי\"```\n",
    "\n",
    "![example_sentences_tagged](img/result_example.png \"sentences_tagged\")\n",
    "\n",
    "Note: This example takes approximately 90 mintues to run on a single GTX 1060 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#!pip install tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import gensim\n",
    "from scipy import spatial\n",
    "\n",
    "orginal_sentences_oath_file = 'Data/Hebrew/SVLM_Hebrew_Wikipedia_Corpus.txt'\n",
    "tagged_sentences_path_file = 'Data/Hebrew/parsed.txt'\n",
    "csv_hebrew_sentences_path_file = 'Data/Hebrew/Hebrew_tagged_sentences.csv'\n",
    "NUM_EXAMPLES = 50000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging and Preprocessing the dataset\n",
    "\n",
    "We'll use a [curpus](https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/blob/master/SVLM_Hebrew_Wikipedia_Corpus.txt) built by Dr. Vered Silber-Varod and Prof. Ami Moyal as part of their work on [paper](https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/blob/master/Phonemes_freqency_Silber-Varod-Latin-Moyal.pdf).\n",
    "\n",
    "The SVLM Hebrew Wikipedia Courpus is a corpus made up of 50,000 Hebrew sentences from the Hebrew Wikipedia chosen to ensure phoneme coverage for the purpose of a sentence recording project\n",
    "\n",
    "This Corpus contains sentence in the format:\n",
    "```\n",
    "פרסומים עיקריים מקורות המחשבה הצבאית המודרנית משרד הביטחון ההוצאה לאור\n",
    "קישורים חיצוניים אתר האינטרנט של המרכז הירושלמי לענייני ציבור ומדינה\n",
    "```\n",
    "As it was generated from Hebrew Wikipedia sources, which are licensed under the [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) license, this corpus is thus also necessarilly licensed under the same license. \n",
    "\n",
    "### tagging\n",
    "\n",
    "To tag the sentences we'll use Hebrew Dependency Parser  [Yoav Goldberg, September 2011](https://www.cs.bgu.ac.il/~yoavg/software/hebparsers/hebdepparser/).\n",
    "\n",
    "For example to the input:\n",
    "```\n",
    "קורות חייו צמרת למד בחוגים לחינוך ופילוסופיה באוניברסיטה העברית בירושלים\n",
    "```\n",
    "the parser will output parsing sentence like this (it will be in the location - 'Data/Hebrew/parsed.txt' ) : \n",
    "\n",
    "![sentences_tagged](img/sentences_tagged.png \"sentences_tagged\")\n",
    "\n",
    "we will take only the subject word (SBJ):![sentences_tagged_SBJ](img/sentences_tagged_sbj.png \"sentences_tagged\")\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. extract subject for each tagged sentence.\n",
    "2. flat the list of subject to one string.\n",
    "3. match subject to orginal sentence.\n",
    "4. make csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "The csv contains sentences & subjects pairs in the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract subject for each tagged sentence\n",
    "def extract_sbj(sentences):\n",
    "    listoflists = []\n",
    "    sublist = []\n",
    "    for i in sentences:\n",
    "        if i.find(\"SBJ\") !=-1:\n",
    "            i = re.sub(r\"[^א-ת\\\"]+\", \" \", i)\n",
    "            if len(i)>3:\n",
    "                sublist.append(i)\n",
    "        elif i == '\\n':\n",
    "            listoflists.append(sublist)\n",
    "            sublist = []\n",
    "    return listoflists\n",
    "\n",
    "#flat the list of subject to one string\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s))\n",
    "\n",
    "#match subject to orginal sentence\n",
    "def make_pairs(orginal_sentences,tagged_sentences,num_examples):\n",
    "    subjects = extract_sbj(tagged_sentences)\n",
    "    clear_lines = []\n",
    "    for tag, sentence in zip(subjects[:num_examples],orginal_sentences[:num_examples]):\n",
    "        clear_lines.append(listToString(tag) + '\\t' + sentence)\n",
    "    subject = []\n",
    "    sentences = []\n",
    "    for line in clear_lines:\n",
    "        line = line.split('\\t')\n",
    "        sentences.append(line[1])\n",
    "        subject.append(line[0])\n",
    "    return subject, sentences\n",
    "\n",
    "def make_csv_subject_sentence(orginal_sentences,tagged_sentences, csv_path,num_examples):\n",
    "    subjects, sentences = make_pairs(orginal_sentences,tagged_sentences,num_examples)\n",
    "    trainDF = pd.DataFrame()\n",
    "    trainDF['subject'] = subjects[:num_examples]\n",
    "    trainDF['sentence'] = sentences[:num_examples]\n",
    "    mask = (trainDF['subject'].str.len()>1)\n",
    "    trainDF = trainDF.loc[mask]\n",
    "    trainDF.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_sentences = open(orginal_sentences_oath_file).readlines()\n",
    "tagged_sentences = open(tagged_sentences_path_file).readlines()\n",
    "make_csv_subject_sentence(orginal_sentences,tagged_sentences,csv_hebrew_sentences_path_file,NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_hebrew_sentences_path_file,index_col=[0])\n",
    "NUM_EXAMPLES = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>קורות</td>\n",
       "      <td>קורות חייו צמרת למד בחוגים לחינוך ופילוסופיה ב...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>פרסומים</td>\n",
       "      <td>פרסומים עיקריים מקורות המחשבה הצבאית המודרנית ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>מרוקו</td>\n",
       "      <td>מרוקו הצטרפה למשפחת האירוויזיון בפעם הראשונה ו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>קו</td>\n",
       "      <td>כתוצאה ממלחמת העצמאות ובעקבות הסכמי שביתת הנשק...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>הסבר</td>\n",
       "      <td>הסבר לחשיבות הקריטריון ניתן למצוא בדיון שנערך ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject                                           sentence\n",
       "0      קורות   קורות חייו צמרת למד בחוגים לחינוך ופילוסופיה ב...\n",
       "3    פרסומים   פרסומים עיקריים מקורות המחשבה הצבאית המודרנית ...\n",
       "7      מרוקו   מרוקו הצטרפה למשפחת האירוויזיון בפעם הראשונה ו...\n",
       "12        קו   כתוצאה ממלחמת העצמאות ובעקבות הסכמי שביתת הנשק...\n",
       "13      הסבר   הסבר לחשיבות הקריטריון ניתן למצוא בדיון שנערך ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing & Cleaning:\n",
    "\n",
    "Here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence_Hebrew(w):\n",
    "    #w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    #w = re.sub(r\"([?.!,¿\\\"])\", r\" \\1 \", w)\n",
    "    #w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^א-ת\\\"]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence_English(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> מבחינתי אפשר להעלות אותו לרשימת ההמתנה לשמוע הערות ואח\"כ להצבעה <end>\n"
     ]
    }
   ],
   "source": [
    "example_sentence = preprocess_sentence_Hebrew(\"מבחינתי אפשר להעלות אותו לרשימת ההמתנה לשמוע הערות ואח\\\"כ להצבעה\")\n",
    "print(example_sentence)\n",
    "example_sentence = [subject for subject in example_sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_csv(csv_path, num_examples):\n",
    "    df = pd.read_csv(csv_path,index_col=[0])\n",
    "    subjects = [preprocess_sentence_Hebrew(subject) for subject in df['subject']]  \n",
    "    sentences = [preprocess_sentence_Hebrew(sentence) for sentence in df['sentence']] \n",
    "    subjects = [subject.split() for subject in subjects]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "    return subjects, sentences,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, sentences,df = create_dataset_from_csv(csv_hebrew_sentences_path_file, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_csv_new(csv_path, num_examples):\n",
    "    df = pd.read_csv(csv_path,index_col=[0])\n",
    "    subjects = df['subject']\n",
    "    sentences = [preprocess_sentence_Hebrew(sentence) for sentence in df['sentence']] \n",
    "    subjects = [subject.split() for subject in subjects]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "    return subjects, sentences,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_100, sentences_100,df_100 = create_dataset_from_csv_new(csv_hebrew_sentences_path_file, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['קורות',\n",
       " 'פרסומים',\n",
       " 'מרוקו',\n",
       " 'קו',\n",
       " 'הסבר',\n",
       " 'פקולטה',\n",
       " 'התנהגות',\n",
       " 'הוא',\n",
       " 'עשה',\n",
       " 'השפעת',\n",
       " 'שרידי',\n",
       " 'מספר',\n",
       " 'דיון',\n",
       " 'ערכים',\n",
       " 'אביטל',\n",
       " 'מאמץ',\n",
       " 'חוקרי',\n",
       " 'גאוגרפיה',\n",
       " 'ריצ',\n",
       " 'אנו',\n",
       " 'פנטום',\n",
       " 'קרטר',\n",
       " 'בריטניה',\n",
       " 'מסחר',\n",
       " 'אינדקס',\n",
       " 'קישורים',\n",
       " 'מדינה',\n",
       " 'טכנולוגיה',\n",
       " 'דיסקוגרפיה',\n",
       " 'קיסר',\n",
       " 'קטגוריית',\n",
       " 'קישורים',\n",
       " 'חסידות',\n",
       " 'ויקיפדיה',\n",
       " 'היסטוריונים',\n",
       " 'חלקן',\n",
       " 'טיל',\n",
       " 'מחשבים',\n",
       " 'הקמת',\n",
       " 'יוצאי',\n",
       " 'לוח',\n",
       " 'ערכים',\n",
       " 'הגדרה',\n",
       " 'בנים',\n",
       " 'ערך',\n",
       " 'עוד',\n",
       " 'הוא',\n",
       " 'כלים',\n",
       " 'יקיחדשות',\n",
       " 'זאת',\n",
       " 'יצירה',\n",
       " 'פתרונות',\n",
       " 'הקמת',\n",
       " 'מדיניות',\n",
       " 'מלחמת',\n",
       " 'נובגורוד',\n",
       " 'חנויות',\n",
       " 'האם',\n",
       " 'תקופה',\n",
       " 'אדולף',\n",
       " 'מנהל',\n",
       " 'צומת',\n",
       " 'קישורים',\n",
       " 'ערך',\n",
       " 'ג',\n",
       " 'חפירות',\n",
       " 'רפורמציה',\n",
       " 'כנסיות',\n",
       " 'ביוגרפיה',\n",
       " 'ערכים',\n",
       " 'ג',\n",
       " 'תואר',\n",
       " 'קשרים',\n",
       " 'הצבעת',\n",
       " 'הצעת',\n",
       " 'לימודי',\n",
       " 'חובבי',\n",
       " 'אקראית',\n",
       " 'הם',\n",
       " 'משפחה',\n",
       " 'מערכת',\n",
       " 'משתמשים',\n",
       " 'אתה',\n",
       " 'הוא',\n",
       " 'מחבר',\n",
       " 'תשומת',\n",
       " 'מדיניות',\n",
       " 'התייחסות',\n",
       " 'ערכים',\n",
       " 'מסמך',\n",
       " 'אלכסנדר',\n",
       " 'החלטת',\n",
       " 'הוא',\n",
       " 'תאוריה',\n",
       " 'פיגועים',\n",
       " 'קיימות',\n",
       " 'קריירה',\n",
       " 'ארגנטינה',\n",
       " 'הכשרה',\n",
       " 'עובדה',\n",
       " 'מסתבר',\n",
       " 'צירים',\n",
       " 'אזרבייג',\n",
       " 'חוק',\n",
       " 'שאלה',\n",
       " 'השפעה',\n",
       " 'ביוגרפיה',\n",
       " 'בית',\n",
       " 'חלק',\n",
       " 'מבקר',\n",
       " 'עמדתו',\n",
       " 'נבחרת',\n",
       " 'נימוק',\n",
       " 'עניין',\n",
       " 'ערכים',\n",
       " 'נפשות',\n",
       " 'נציג',\n",
       " 'משחק',\n",
       " 'קישורים',\n",
       " 'התנהגות',\n",
       " 'גאוגרפיה',\n",
       " 'גורמים',\n",
       " 'שי',\n",
       " 'בעיה',\n",
       " 'עדכונים',\n",
       " 'קישורים',\n",
       " 'אזורים',\n",
       " 'עיר',\n",
       " 'ויקיפדיה',\n",
       " 'עיריית',\n",
       " 'חוסר',\n",
       " 'הצבעות',\n",
       " 'יצחק',\n",
       " 'חברי',\n",
       " 'עצם',\n",
       " 'גורמים',\n",
       " 'צרפתים',\n",
       " 'חיים',\n",
       " 'אלכסנדר',\n",
       " 'היסטוריה',\n",
       " 'תפקיד',\n",
       " 'דיון',\n",
       " 'קטעים',\n",
       " 'רעיון',\n",
       " 'אנציקלופדיה',\n",
       " 'חבר',\n",
       " 'ישראל',\n",
       " 'פתיחה',\n",
       " 'אהב',\n",
       " 'סטודנטים',\n",
       " 'אני',\n",
       " 'החלה',\n",
       " 'הבה',\n",
       " 'הוא',\n",
       " 'זה',\n",
       " 'ערך',\n",
       " 'קישורים',\n",
       " 'בריטניה',\n",
       " 'בסיס',\n",
       " 'יועץ',\n",
       " 'מגמה',\n",
       " 'בחירה',\n",
       " 'צ',\n",
       " 'מערכת',\n",
       " 'סן',\n",
       " 'קישורים',\n",
       " 'סניפים',\n",
       " 'סבורים',\n",
       " 'דיון',\n",
       " 'בויקיפדיה',\n",
       " 'פורטל',\n",
       " 'משפחות',\n",
       " 'מערכת',\n",
       " 'כתיבה',\n",
       " 'ניב',\n",
       " 'אתר',\n",
       " 'ערכים',\n",
       " 'מאמרי',\n",
       " 'שחקנים',\n",
       " 'ארגון',\n",
       " 'נסיכה',\n",
       " 'נבחרת',\n",
       " 'נבחרות',\n",
       " 'ספרים',\n",
       " 'עצם',\n",
       " 'הוא',\n",
       " 'עצמון',\n",
       " 'אבידן',\n",
       " 'הוא',\n",
       " 'נשק',\n",
       " 'אום',\n",
       " 'דיון',\n",
       " 'ויקיציטוט',\n",
       " 'יפתח',\n",
       " 'אני',\n",
       " 'קישורים',\n",
       " 'משפחות',\n",
       " 'אצ\"ל',\n",
       " 'חוקים',\n",
       " 'מבנה',\n",
       " 'עיר',\n",
       " 'ייצור',\n",
       " 'חשיבות',\n",
       " 'קישורים',\n",
       " 'שרידי',\n",
       " 'משתמשים',\n",
       " 'אביו',\n",
       " 'ביוגרפיה',\n",
       " 'עמדתו',\n",
       " 'נסיונות',\n",
       " 'עובדה',\n",
       " 'בעיה',\n",
       " 'הצעה',\n",
       " 'ספרות',\n",
       " 'סיירת',\n",
       " 'אנחנו',\n",
       " 'מועדון',\n",
       " 'שגריר',\n",
       " 'תופעה',\n",
       " 'זאת',\n",
       " 'זהו',\n",
       " 'מבקרי',\n",
       " 'נקודות',\n",
       " 'עיקר',\n",
       " 'צ',\n",
       " 'ראשית',\n",
       " 'תאריך',\n",
       " 'תורת',\n",
       " 'תפקידים',\n",
       " 'תקופה',\n",
       " 'פרופסור',\n",
       " 'אלבום',\n",
       " 'דף',\n",
       " 'מחבלים',\n",
       " 'מטרה',\n",
       " 'קישורים',\n",
       " 'תוצאות',\n",
       " 'מרכזים',\n",
       " 'בודהיזם',\n",
       " 'עיתון',\n",
       " 'היא',\n",
       " 'רשות',\n",
       " 'ממשלת',\n",
       " 'לוגיקה',\n",
       " 'בדוק',\n",
       " 'ממלכה',\n",
       " 'סמל',\n",
       " 'שימוש',\n",
       " 'הן',\n",
       " 'הסתדרות',\n",
       " 'קישורים',\n",
       " 'שחקנים',\n",
       " 'אתר',\n",
       " 'דיונים',\n",
       " 'הוא',\n",
       " 'כולם',\n",
       " 'אוניברסיטה',\n",
       " 'הרקולס',\n",
       " 'חפץ',\n",
       " 'אנסה',\n",
       " 'רבי',\n",
       " 'יומי',\n",
       " 'ישראל',\n",
       " 'נוסח',\n",
       " 'של',\n",
       " 'קישורים',\n",
       " 'מונחים',\n",
       " 'שם',\n",
       " 'אני',\n",
       " 'זהו',\n",
       " 'החלטה',\n",
       " 'טענה',\n",
       " 'הבוט',\n",
       " 'חסינות',\n",
       " 'אורכו',\n",
       " 'שנה',\n",
       " 'הוא',\n",
       " 'רעיון',\n",
       " 'יהודים',\n",
       " 'אני',\n",
       " 'ערכים',\n",
       " 'מספר',\n",
       " 'גידול',\n",
       " 'מדיניות',\n",
       " 'ספר',\n",
       " 'צבא',\n",
       " 'זמן',\n",
       " 'חבל',\n",
       " 'התנגדות',\n",
       " 'היכן',\n",
       " 'הפרה',\n",
       " 'גידול',\n",
       " 'קישורים',\n",
       " 'ערך',\n",
       " 'אתה',\n",
       " 'מכללה',\n",
       " 'דבר',\n",
       " 'חבר',\n",
       " 'טרול',\n",
       " 'ממלכה',\n",
       " 'כול',\n",
       " 'דרך',\n",
       " 'סיבוב',\n",
       " 'פנסילבניה',\n",
       " 'תוצאות',\n",
       " 'מילים',\n",
       " 'אני',\n",
       " 'ציד',\n",
       " 'תושבים',\n",
       " 'ארבעה',\n",
       " 'ארגון',\n",
       " 'הוא',\n",
       " 'מתחם',\n",
       " 'תזה',\n",
       " 'מערכת',\n",
       " 'ערכים',\n",
       " 'סמכויות',\n",
       " 'אחיו',\n",
       " 'אלכס',\n",
       " 'פרמטרים',\n",
       " 'הם',\n",
       " 'דבר',\n",
       " 'הופעתם',\n",
       " 'הסברים',\n",
       " 'הצבעה',\n",
       " 'כמדומני',\n",
       " 'מדיניות',\n",
       " 'שאלה',\n",
       " 'רבי',\n",
       " 'הם',\n",
       " 'ספרים',\n",
       " 'אנונימי',\n",
       " 'מטרתו',\n",
       " 'אוניברסיטה',\n",
       " 'שמאל',\n",
       " 'נבחרת',\n",
       " 'שלום',\n",
       " 'מגמה',\n",
       " 'מטבעות',\n",
       " 'ברית',\n",
       " 'אסלאם',\n",
       " 'ציור',\n",
       " 'עשה',\n",
       " 'אנדרטה',\n",
       " 'מנצ',\n",
       " 'מונח',\n",
       " 'נחש',\n",
       " 'צבא',\n",
       " 'ציור',\n",
       " 'יהודים',\n",
       " 'אנא',\n",
       " 'תקציר',\n",
       " 'גבעה',\n",
       " 'ערך',\n",
       " 'יעד',\n",
       " 'התייחסות',\n",
       " 'מי',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'הורביץ',\n",
       " 'ספר',\n",
       " 'קרב',\n",
       " 'משחקי',\n",
       " 'אנשים',\n",
       " 'ריצ',\n",
       " 'צורך',\n",
       " 'אכילה',\n",
       " 'פתיחות',\n",
       " 'טכנולוגיה',\n",
       " 'עדיף',\n",
       " 'פיליפוס',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'אלבומים',\n",
       " 'נתיבי',\n",
       " 'הוא',\n",
       " 'פוליטיקאים',\n",
       " 'הוא',\n",
       " 'אדולף',\n",
       " 'שרידי',\n",
       " 'דמויות',\n",
       " 'אוסף',\n",
       " 'כרך',\n",
       " 'מחלוקת',\n",
       " 'משרד',\n",
       " 'סדרה',\n",
       " 'ערכים',\n",
       " 'טנזניה',\n",
       " 'יצירתו',\n",
       " 'קהילת',\n",
       " 'רוזוולט',\n",
       " 'ערך',\n",
       " 'אנחנו',\n",
       " 'מרד',\n",
       " 'קונצנזוס',\n",
       " 'תווי',\n",
       " 'התנגדות',\n",
       " 'איסוף',\n",
       " 'איציק',\n",
       " 'אנחנו',\n",
       " 'אספקת',\n",
       " 'בעיה',\n",
       " 'השתתפות',\n",
       " 'וועד',\n",
       " 'מבנים',\n",
       " 'משפחות',\n",
       " 'ספר',\n",
       " 'תצפית',\n",
       " 'עזר',\n",
       " 'חשיבות',\n",
       " 'עבודתה',\n",
       " 'עסק',\n",
       " 'קישורים',\n",
       " 'אנו',\n",
       " 'גזר',\n",
       " 'עובדה',\n",
       " 'תוכנית',\n",
       " 'יצחקי',\n",
       " 'היא',\n",
       " 'גאולוגיה',\n",
       " 'מדינה',\n",
       " 'קישור',\n",
       " 'סדרת',\n",
       " 'סלומון',\n",
       " 'קריירת',\n",
       " 'הוספתי',\n",
       " 'השפעתם',\n",
       " 'תפקיד',\n",
       " 'עיצוב',\n",
       " 'קישורים',\n",
       " 'מכונית',\n",
       " 'ידיעות',\n",
       " 'אוניברסיטת',\n",
       " 'נבחרת',\n",
       " 'שגיאות',\n",
       " 'תפקוד',\n",
       " 'ליגה',\n",
       " 'אלמנטים',\n",
       " 'ילדים',\n",
       " 'מטרה',\n",
       " 'עשרה',\n",
       " 'מערך',\n",
       " 'התייחסות',\n",
       " 'חברי',\n",
       " 'מסחר',\n",
       " 'יתרה',\n",
       " 'אגף',\n",
       " 'קישורים',\n",
       " 'שטח',\n",
       " 'סרטי',\n",
       " 'צורה',\n",
       " 'אריסטו',\n",
       " 'מנגנון',\n",
       " 'להקה',\n",
       " 'מספר',\n",
       " 'שנייה',\n",
       " 'חייו',\n",
       " 'מחיקת',\n",
       " 'קישורים',\n",
       " 'אהרן',\n",
       " 'לניו',\n",
       " 'אלה',\n",
       " 'צעירי',\n",
       " 'נחנך',\n",
       " 'הוא',\n",
       " 'סיבה',\n",
       " 'תקליט',\n",
       " 'בעיה',\n",
       " 'פרקים',\n",
       " 'אני',\n",
       " 'מדינה',\n",
       " 'מסתבר',\n",
       " 'תושבי',\n",
       " 'אני',\n",
       " 'חיבורים',\n",
       " 'חפירות',\n",
       " 'בני',\n",
       " 'אני',\n",
       " 'היסטוריונים',\n",
       " 'יועץ',\n",
       " 'ילד',\n",
       " 'מפעלים',\n",
       " 'הצעתי',\n",
       " 'קרב',\n",
       " 'זהו',\n",
       " 'אחוז',\n",
       " 'להקה',\n",
       " 'התייחסות',\n",
       " 'הן',\n",
       " 'מלחמה',\n",
       " 'מנהל',\n",
       " 'מספר',\n",
       " 'אפשרות',\n",
       " 'אירועים',\n",
       " 'מצרים',\n",
       " 'היטלר',\n",
       " 'מנחה',\n",
       " 'תלמידים',\n",
       " 'יהודים',\n",
       " 'מבנה',\n",
       " 'מידע',\n",
       " 'מלך',\n",
       " 'שימוש',\n",
       " 'בקר',\n",
       " 'חלוקה',\n",
       " 'מלון',\n",
       " 'מונטגומרי',\n",
       " 'משתמש',\n",
       " 'ערך',\n",
       " 'עריכות',\n",
       " 'תמונה',\n",
       " 'ספרי',\n",
       " 'קישורים',\n",
       " 'יישובים',\n",
       " 'ים',\n",
       " 'מקדונלד',\n",
       " 'זו',\n",
       " 'מניעה',\n",
       " 'אלבום',\n",
       " 'בניו',\n",
       " 'תיבה',\n",
       " 'מנהגים',\n",
       " 'כך',\n",
       " 'ספרות',\n",
       " 'זה',\n",
       " 'איינשטיין',\n",
       " 'מקסוול',\n",
       " 'עמוד',\n",
       " 'תגלית',\n",
       " 'אוניית',\n",
       " 'יהודים',\n",
       " 'משבר',\n",
       " 'זו',\n",
       " 'מתיחות',\n",
       " 'מנזר',\n",
       " 'השפעתה',\n",
       " 'ויקיפדים',\n",
       " 'חברי',\n",
       " 'דעות',\n",
       " 'היא',\n",
       " 'הם',\n",
       " 'מפקדים',\n",
       " 'סובייטים',\n",
       " 'ספר',\n",
       " 'שניה',\n",
       " 'חוסר',\n",
       " 'פרסום',\n",
       " 'חברה',\n",
       " 'נבחרת',\n",
       " 'נבחרת',\n",
       " 'מירוץ',\n",
       " 'היא',\n",
       " 'נבחרת',\n",
       " 'אתה',\n",
       " 'מקור',\n",
       " 'קבוצה',\n",
       " 'אוניברסיטת',\n",
       " 'נכנסתי',\n",
       " 'משחק',\n",
       " 'סרט',\n",
       " 'זאב',\n",
       " 'תוצאות',\n",
       " 'הוא',\n",
       " 'ציונית',\n",
       " 'ממלכת',\n",
       " 'תרגום',\n",
       " 'חשיפה',\n",
       " 'אתה',\n",
       " 'זו',\n",
       " 'חזרת',\n",
       " 'חוסר',\n",
       " 'טכנולוגיה',\n",
       " 'אנו',\n",
       " 'ישראל',\n",
       " 'צבא',\n",
       " 'ידיעה',\n",
       " 'קיד',\n",
       " 'פיינברג',\n",
       " 'סגנון',\n",
       " 'דבר',\n",
       " 'שחייה',\n",
       " 'חפצים',\n",
       " 'בית',\n",
       " 'מישהו',\n",
       " 'כמות',\n",
       " 'שלושה',\n",
       " 'מהלך',\n",
       " 'מסע',\n",
       " 'רופאים',\n",
       " 'תשובה',\n",
       " 'אני',\n",
       " 'קישורים',\n",
       " 'רובם',\n",
       " 'שלט',\n",
       " 'אופציה',\n",
       " 'קרבות',\n",
       " 'משפחה',\n",
       " 'נוצרה',\n",
       " 'ארצות',\n",
       " 'חלקים',\n",
       " 'ספר',\n",
       " 'פרדוקס',\n",
       " 'אני',\n",
       " 'סיפור',\n",
       " 'חיילי',\n",
       " 'מצב',\n",
       " 'מקצועיות',\n",
       " 'רצונם',\n",
       " 'אזורים',\n",
       " 'החליטו',\n",
       " 'צדדים',\n",
       " 'אני',\n",
       " 'אפשרות',\n",
       " 'שנים',\n",
       " 'יפו',\n",
       " 'אפשרויות',\n",
       " 'ביקור',\n",
       " 'בית',\n",
       " 'ביצועים',\n",
       " 'חלק',\n",
       " 'ניצחון',\n",
       " 'ברקאי',\n",
       " 'בן',\n",
       " 'כוחות',\n",
       " 'עלייתה',\n",
       " 'דור',\n",
       " 'ביקורת',\n",
       " 'דמויות',\n",
       " 'ביקור',\n",
       " 'מפורסם',\n",
       " 'נוהל',\n",
       " 'שאלה',\n",
       " 'זה',\n",
       " 'חריגה',\n",
       " 'התפתחות',\n",
       " 'מניע',\n",
       " 'רכבת',\n",
       " 'הקפדה',\n",
       " 'מדינה',\n",
       " 'מנסל',\n",
       " 'מעמדה',\n",
       " 'מתנגדי',\n",
       " 'סדר',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'שמונים',\n",
       " 'אנחנו',\n",
       " 'הוא',\n",
       " 'קווי',\n",
       " 'מספר',\n",
       " 'חלק',\n",
       " 'מעבר',\n",
       " 'קטלוניה',\n",
       " 'מנחה',\n",
       " 'גאוגרפיה',\n",
       " 'הוא',\n",
       " 'הוא',\n",
       " 'חפירות',\n",
       " 'מחלקה',\n",
       " 'תסריט',\n",
       " 'יצירת',\n",
       " 'רמזים',\n",
       " 'הצבעת',\n",
       " 'התנגדות',\n",
       " 'זה',\n",
       " 'יקיפדיה',\n",
       " 'עצם',\n",
       " 'אני',\n",
       " 'שטח',\n",
       " 'שטח',\n",
       " 'ערך',\n",
       " 'אלו',\n",
       " 'אני',\n",
       " 'מלחינים',\n",
       " 'מטרת',\n",
       " 'אוכלוסייה',\n",
       " 'ארגון',\n",
       " 'משרד',\n",
       " 'וועידה',\n",
       " 'מאמצים',\n",
       " 'מחקר',\n",
       " 'מפלגה',\n",
       " 'ספר',\n",
       " 'התנגשויות',\n",
       " 'תרשים',\n",
       " 'הוא',\n",
       " 'זוהי',\n",
       " 'חוק',\n",
       " 'קשרים',\n",
       " 'קישורים',\n",
       " 'אירועי',\n",
       " 'אליוט',\n",
       " 'תפקידים',\n",
       " 'תואר',\n",
       " 'אני',\n",
       " 'להלחם',\n",
       " 'היסטוריה',\n",
       " 'הסדרה',\n",
       " 'עקרון',\n",
       " 'מקום',\n",
       " 'חוקרי',\n",
       " 'לשון',\n",
       " 'מספר',\n",
       " 'אבו',\n",
       " 'אני',\n",
       " 'ויזואלית',\n",
       " 'ערך',\n",
       " 'עמוד',\n",
       " 'קישורים',\n",
       " 'רבים',\n",
       " 'ערך',\n",
       " 'ביקורות',\n",
       " 'שרון',\n",
       " 'סיפור',\n",
       " 'אוכלוסייה',\n",
       " 'צורת',\n",
       " 'מסקנות',\n",
       " 'מקור',\n",
       " 'תקופת',\n",
       " 'עידוד',\n",
       " 'דוד',\n",
       " 'מכתש',\n",
       " 'הקלטת',\n",
       " 'נוסף',\n",
       " 'ניתוח',\n",
       " 'עמוד',\n",
       " 'שאלת',\n",
       " 'לקוחות',\n",
       " 'ניסיונות',\n",
       " 'קהילה',\n",
       " 'מבנה',\n",
       " 'שאיפה',\n",
       " 'בויקיפדיה',\n",
       " 'אירועים',\n",
       " 'מבנה',\n",
       " 'אלקטרוניקה',\n",
       " 'ליגה',\n",
       " 'רקע',\n",
       " 'מהדורה',\n",
       " 'משחק',\n",
       " 'צה\"ל',\n",
       " 'פרטים',\n",
       " 'חברי',\n",
       " 'מד',\n",
       " 'הוא',\n",
       " 'משחק',\n",
       " 'משפחות',\n",
       " 'ערך',\n",
       " 'ויקיפדיה',\n",
       " 'שימוש',\n",
       " 'מרבית',\n",
       " 'קישורים',\n",
       " 'ששון',\n",
       " 'אודה',\n",
       " 'אלכסנדר',\n",
       " 'אני',\n",
       " 'בית',\n",
       " 'תחום',\n",
       " 'ג',\n",
       " 'אמריקאים',\n",
       " 'הפעלת',\n",
       " 'צמח',\n",
       " 'קהילה',\n",
       " 'מושבה',\n",
       " 'ויליאמס',\n",
       " 'זה',\n",
       " 'אזרחי',\n",
       " 'פעולות',\n",
       " 'הוא',\n",
       " 'לבנון',\n",
       " 'מבצע',\n",
       " 'הבנת',\n",
       " 'מיקום',\n",
       " 'משה',\n",
       " 'עיתון',\n",
       " 'פרויקט',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'דיונים',\n",
       " 'דבר',\n",
       " 'אתר',\n",
       " 'מנהל',\n",
       " 'גרשם',\n",
       " 'מבצע',\n",
       " 'מידע',\n",
       " 'טיסות',\n",
       " 'בנוסף',\n",
       " 'דוגמה',\n",
       " 'אופציה',\n",
       " 'אלבום',\n",
       " 'הצבעה',\n",
       " 'מנוע',\n",
       " 'המצאה',\n",
       " 'משחק',\n",
       " 'הקרנה',\n",
       " 'התברר',\n",
       " 'אנחנו',\n",
       " 'אני',\n",
       " 'שטרן',\n",
       " 'כתבת',\n",
       " 'קישורים',\n",
       " 'תוצאות',\n",
       " 'משחקים',\n",
       " 'עת',\n",
       " 'גלריה',\n",
       " 'תפילת',\n",
       " 'חברים',\n",
       " 'גשמים',\n",
       " 'דיין',\n",
       " 'גרסה',\n",
       " 'הוא',\n",
       " 'הוא',\n",
       " 'כתיבה',\n",
       " 'מספר',\n",
       " 'פולנים',\n",
       " 'ז',\n",
       " 'חלקו',\n",
       " 'הבדל',\n",
       " 'נזק',\n",
       " 'התחלתי',\n",
       " 'ערך',\n",
       " 'מקום',\n",
       " 'מישהו',\n",
       " 'מיעוט',\n",
       " 'נתונים',\n",
       " 'ניקסון',\n",
       " 'דיוני',\n",
       " 'עמוד',\n",
       " 'עמוד',\n",
       " 'צפון',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'שחקנים',\n",
       " 'אמצעי',\n",
       " 'אני',\n",
       " 'אשקלון',\n",
       " 'אני',\n",
       " 'צורך',\n",
       " 'תומפסון',\n",
       " 'בעיה',\n",
       " 'הפניות',\n",
       " 'התקדמות',\n",
       " 'היסטוריה',\n",
       " 'מצב',\n",
       " 'משמעות',\n",
       " 'סימפוניה',\n",
       " 'ערך',\n",
       " 'הצעה',\n",
       " 'קטגוריה',\n",
       " 'תוצאה',\n",
       " 'מקום',\n",
       " 'אם',\n",
       " 'החלטה',\n",
       " 'זכותך',\n",
       " 'בחירות',\n",
       " 'אנחנו',\n",
       " 'טענה',\n",
       " 'בית',\n",
       " 'מחלקת',\n",
       " 'מיה',\n",
       " 'ערכים',\n",
       " 'מחלוקת',\n",
       " 'קישורים',\n",
       " 'פעולות',\n",
       " 'אנחנו',\n",
       " 'ביוגרפיה',\n",
       " 'שלישים',\n",
       " 'ויקיפדיה',\n",
       " 'גרסא',\n",
       " 'קהילה',\n",
       " 'קרבה',\n",
       " 'יחס',\n",
       " 'הסטוריה',\n",
       " 'בעיה',\n",
       " 'יסודות',\n",
       " 'לינק',\n",
       " 'דיונים',\n",
       " 'ממצא',\n",
       " 'מספר',\n",
       " 'עמוד',\n",
       " 'עמוד',\n",
       " 'עמוד',\n",
       " 'עמוד',\n",
       " 'עריכה',\n",
       " 'קטלוג',\n",
       " 'תורן',\n",
       " 'כנסיית',\n",
       " 'אורי',\n",
       " 'אתר',\n",
       " 'בנו',\n",
       " 'דחפורים',\n",
       " 'כיבוש',\n",
       " 'סדרה',\n",
       " 'שחקן',\n",
       " 'חברי',\n",
       " 'שרידים',\n",
       " 'כול',\n",
       " 'יהודים',\n",
       " 'גולשים',\n",
       " 'קיים',\n",
       " 'מסתבר',\n",
       " 'עבודת',\n",
       " 'אני',\n",
       " 'משמעות',\n",
       " 'חלק',\n",
       " 'קולינס',\n",
       " 'שרף',\n",
       " 'תלמידי',\n",
       " 'דרישת',\n",
       " 'דעות',\n",
       " 'אתה',\n",
       " 'הרולד',\n",
       " 'נוספים',\n",
       " 'החלטות',\n",
       " 'עיתונים',\n",
       " 'נבחרת',\n",
       " 'אני',\n",
       " 'אלה',\n",
       " 'מועדון',\n",
       " 'מלכים',\n",
       " 'מרבית',\n",
       " 'נבחרת',\n",
       " 'ערכים',\n",
       " 'קורא',\n",
       " 'קישורים',\n",
       " 'ערכים',\n",
       " 'סטודנטים',\n",
       " 'נבחרת',\n",
       " 'אולימפיאדת',\n",
       " 'הפניה',\n",
       " 'מומלץ',\n",
       " 'מדינת',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'ערך',\n",
       " 'שמות',\n",
       " 'וועדה',\n",
       " 'קישורים',\n",
       " 'הוצאה',\n",
       " 'דוד',\n",
       " 'מקום',\n",
       " 'קישור',\n",
       " 'שלטון',\n",
       " 'היא',\n",
       " 'מובן',\n",
       " 'ראיתי',\n",
       " 'לאו',\n",
       " 'אנחנו',\n",
       " 'אנשי',\n",
       " 'סרט',\n",
       " 'תעשיית',\n",
       " 'בעיה',\n",
       " 'הצעה',\n",
       " 'ויכוח',\n",
       " 'קטגוריות',\n",
       " 'שנה',\n",
       " 'שחקני',\n",
       " 'ברוקולי',\n",
       " 'פליטים',\n",
       " 'קישורים',\n",
       " 'קישורים',\n",
       " 'אגם',\n",
       " 'אני',\n",
       " 'אני',\n",
       " 'אתה',\n",
       " 'מפעיל',\n",
       " 'בנו',\n",
       " 'הדרגתית',\n",
       " 'אי',\n",
       " 'אנחנו',\n",
       " 'בוגרים',\n",
       " 'בעייה',\n",
       " 'טבח',\n",
       " 'שיח',\n",
       " 'תקליט',\n",
       " 'טענתי',\n",
       " 'טקסט',\n",
       " 'מקום',\n",
       " 'היא',\n",
       " 'משחקי',\n",
       " 'סקרלט',\n",
       " 'ערך',\n",
       " 'פרטים',\n",
       " 'קישורים',\n",
       " 'שירות',\n",
       " 'פלגים',\n",
       " 'אוליבר',\n",
       " 'איש',\n",
       " 'אני',\n",
       " 'קרבות',\n",
       " 'דבר',\n",
       " 'פרויקט',\n",
       " 'הוא',\n",
       " 'מסלול',\n",
       " 'שנייה',\n",
       " 'איים',\n",
       " 'איש',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ss = [a[0] for a in subjects_100]\n",
    "new_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec(subjects+sentences, size=EMBEDDING_DIM, min_count=1, window=5, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [word_model.wv[a] for a in new_ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.DataFrame()\n",
    "trainDF['vector'] = vectors\n",
    "trainDF['subject'] = new_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00398569, -1.8901986 , -0.04167079,  0.5108059 , -0.69707596,\n",
       "        0.40042928, -1.231232  , -1.6347198 ,  0.09509303, -1.2260326 ,\n",
       "       -0.03563622,  1.1412473 ,  0.9325855 , -0.50731283,  0.73774457,\n",
       "       -0.18820085, -0.57643193,  0.6506263 ,  0.4773282 , -0.22409096,\n",
       "       -0.37280095,  0.87427264, -0.22967365, -0.91149324,  0.32480574,\n",
       "        0.80843794, -0.9488527 ,  1.5277755 ,  1.1899632 , -0.31318113,\n",
       "        1.2684431 , -0.5371515 ,  0.12485521,  1.4042177 , -0.10658001,\n",
       "       -1.1739335 ,  0.06208733, -0.27067086,  0.44054776, -0.9853829 ,\n",
       "        0.30536523,  0.45870048,  1.0198349 ,  0.57741475,  0.45780244,\n",
       "       -0.39679253, -2.4860334 , -0.4869366 , -0.36467692,  0.11694994,\n",
       "       -1.1362572 , -0.48502183, -0.7076488 ,  0.09906752, -0.6212556 ,\n",
       "        0.33545193,  0.51957005,  1.006085  ,  0.18142621, -0.5473227 ,\n",
       "       -0.4349027 ,  1.7716393 , -0.4017008 ,  0.13678183,  0.81687164,\n",
       "        0.097845  ,  0.19195746,  0.98078406, -0.50224024, -0.21202628,\n",
       "        1.2975396 , -0.24997677, -0.01704272, -0.584053  , -0.24624847,\n",
       "        0.29978165, -0.11601949,  0.57746017, -0.7649031 , -0.57180494,\n",
       "        0.44782868, -1.7344034 , -1.0514214 , -0.16318648, -0.81407714,\n",
       "       -0.66114485, -0.7605728 , -0.35695112, -0.399025  ,  1.4813056 ,\n",
       "        0.05346967,  0.8329858 , -0.35151076,  0.49888122,  0.3766387 ,\n",
       "       -0.05834441,  0.43029267,  0.8634932 ,  0.77627015, -0.4688623 ,\n",
       "       -0.3533424 ,  0.5880432 , -1.4508009 , -1.3977207 ,  0.4470124 ,\n",
       "       -0.48703015, -0.19797906, -0.74661297,  0.9463738 , -0.5874214 ,\n",
       "        0.50494736,  1.087522  ,  0.6414483 , -0.41007176,  0.92831546,\n",
       "       -0.65983874, -0.47226265,  0.52352166, -0.23708247, -1.0719231 ,\n",
       "       -0.19312596, -0.02482868, -0.8988178 ,  0.16532023, -0.2848625 ,\n",
       "       -0.33528683, -0.46300578, -0.38623238, -0.8260395 , -0.30252442,\n",
       "       -1.491626  ,  2.3330786 ,  1.3346426 , -0.02898894, -1.5759532 ,\n",
       "        0.29347545, -0.56128454, -0.58757484, -0.12561442,  0.6258466 ,\n",
       "        0.07206971, -0.17632727, -0.74862486,  0.32697728,  0.36472443,\n",
       "       -1.074087  , -0.08649639,  1.6154557 ,  1.3974887 , -0.21706618,\n",
       "       -0.5031367 ,  0.07672352,  0.40098158,  0.83744305,  0.00374048,\n",
       "        1.7075288 ,  0.34902856,  0.4611845 ,  1.5583718 ,  1.6860532 ,\n",
       "        0.708805  ,  0.03546664, -0.83426625, -0.7108404 ,  1.0279325 ,\n",
       "        1.7870374 , -0.45167395, -0.608526  , -0.80468005, -1.535536  ,\n",
       "        0.06497318, -1.0666507 , -0.6888851 ,  0.30375767, -0.43278036,\n",
       "       -1.678783  , -2.3899424 ,  0.2592035 , -0.59341097,  0.3859174 ,\n",
       "       -0.56540036, -0.6814599 , -0.64762235, -0.43195665,  0.25351733,\n",
       "        0.27234122,  0.02411667,  0.2099972 , -0.02320047, -0.2538956 ,\n",
       "        0.11263476, -0.49519065, -0.28434   ,  0.29077417,  0.6879014 ,\n",
       "        0.2914211 , -1.405312  ,  0.33735138,  0.8441386 ,  0.9232865 ,\n",
       "       -0.8847666 , -0.07593182,  0.11395773, -1.0005664 ,  0.55317056,\n",
       "        0.8261425 , -0.73689526, -0.7767592 ,  0.6861097 ,  0.23626193,\n",
       "        0.46642426, -0.06029081, -0.27166852, -0.21792357, -0.09320781,\n",
       "       -0.8439336 ,  1.218491  , -0.75688666, -0.9385511 , -0.17476933,\n",
       "        0.6400574 , -1.7781324 ,  1.9828222 ,  0.67673606,  0.39927068,\n",
       "        0.10964537, -1.6854519 , -1.33746   ,  0.13474464, -0.13125278,\n",
       "       -0.37588587,  0.48824638, -0.36111456,  1.721628  , -0.6693675 ,\n",
       "       -1.6270366 ,  0.8759612 ,  0.8940855 , -0.46127313, -0.16418396,\n",
       "        0.3761365 , -1.1528163 , -0.2246234 , -0.08060919, -0.6677748 ,\n",
       "        0.4553166 , -0.58205676, -0.7806379 ,  0.42776102,  0.42935374,\n",
       "       -0.10971054, -0.15998814,  0.08936287, -0.80328184, -1.0618337 ,\n",
       "       -0.59535843, -1.1448776 ,  0.45729426,  1.3175145 , -1.4839212 ,\n",
       "        0.07794948, -0.58550113,  1.0744953 , -0.48188403,  0.17857613,\n",
       "       -1.359863  , -0.7528537 , -0.13317041,  0.36095414,  0.04819373,\n",
       "       -0.20317474, -1.0774416 ,  0.00489448,  0.5015986 ,  0.46904716,\n",
       "       -0.59737283,  1.0517311 , -0.9842532 ,  0.08455537, -0.57334644,\n",
       "        0.14442573,  0.2533669 ,  0.49305463,  0.42649537,  0.09632576,\n",
       "        0.57342327, -0.91137964, -0.8202171 ,  0.02368088,  0.6396172 ,\n",
       "       -0.7105896 ,  0.17006809,  0.3976147 , -0.11060499, -0.04946726,\n",
       "       -1.1402394 ,  0.45839664,  0.24455197,  1.4372299 , -0.12580438],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['vector'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_csv_path = \"Data/Hebrew/Subjects_vector.csv\"\n",
    "# trainDF.to_csv(new_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:00<00:00, 432.14it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 444.90it/s][A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "Txt_file = \"Data/Hebrew/Subjects_vector.txt\"\n",
    "f = open(Txt_file, \"a\")\n",
    "for idx,i in enumerate(tqdm(vectors[:100])):\n",
    "    f.write(\"THE SUJECT: \"+ new_ss[idx]+'/n')\n",
    "    f.write('/n')\n",
    "    f.write('/n')\n",
    "    f.write(\"VECTOR START >>>>>>>>>>>>>>>\"+ str(i)+\"VECTOR END<<<<<<<<<<<<<<\"+'/n')\n",
    "    f.write('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.6848079, -0.013529115, -0.4853551, -1.30793...</td>\n",
       "      <td>קורות</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.3767081, 0.03185713, -0.5402789, -0.347758...</td>\n",
       "      <td>פרסומים</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.042412773, -0.017034844, 0.1894208, -0.136...</td>\n",
       "      <td>מרוקו</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.7955116, -0.111858465, -0.1133503, 0.665261...</td>\n",
       "      <td>קו</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-1.663684, 0.344732, 0.8169256, -0.73074293, ...</td>\n",
       "      <td>הסבר</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.013377348, 0.011749349, -0.051014815, -0.0...</td>\n",
       "      <td>פקולטה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.4862766, 0.11782753, 0.7233255, 0.610386, ...</td>\n",
       "      <td>התנהגות</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.052913904, -0.62564147, 0.93190145, 0.2648...</td>\n",
       "      <td>הוא</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.0073105013, -0.8763988, 0.6765104, 0.274296...</td>\n",
       "      <td>עשה</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.53223413, -0.23286606, -0.14494942, -0.1577...</td>\n",
       "      <td>השפעת</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  subject\n",
       "0  [1.6848079, -0.013529115, -0.4853551, -1.30793...    קורות\n",
       "1  [-0.3767081, 0.03185713, -0.5402789, -0.347758...  פרסומים\n",
       "2  [-0.042412773, -0.017034844, 0.1894208, -0.136...    מרוקו\n",
       "3  [0.7955116, -0.111858465, -0.1133503, 0.665261...       קו\n",
       "4  [-1.663684, 0.344732, 0.8169256, -0.73074293, ...     הסבר\n",
       "5  [-0.013377348, 0.011749349, -0.051014815, -0.0...   פקולטה\n",
       "6  [-0.4862766, 0.11782753, 0.7233255, 0.610386, ...  התנהגות\n",
       "7  [-0.052913904, -0.62564147, 0.93190145, 0.2648...      הוא\n",
       "8  [0.0073105013, -0.8763988, 0.6765104, 0.274296...      עשה\n",
       "9  [0.53223413, -0.23286606, -0.14494942, -0.1577...    השפעת"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation: evaluating estimator performance\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `x_test, y_test`. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "*Grid Search Workflow*\n",
    "\n",
    "In scikit-learn a random split into training and test sets can be quickly computed with the **`train_test_split`** helper function.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "#### arrays:\n",
    "sequence of indexables with same length / shape[0]\n",
    "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "\n",
    "#### test_size:\n",
    "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n",
    "\n",
    "Let’s load the tagged sentencses data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, subjects_train, subjects_test =\\\n",
    "train_test_split(sentences, subjects, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step. So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "### Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "`word_model = gensim.models.Word2Vec(subjects+sentences, size=EMBEDDING_DIM, min_count=1, window=5, iter=100)`\n",
    "\n",
    "#### size\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "#### window\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you \n",
    "have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1600/0*1uA0SYcKU_dLTj-V.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<center>C is the window size</center>\n",
    "\n",
    "\n",
    "#### min_count\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "#### workers\n",
    "How many threads to use behind the scenes?\n",
    "\n",
    "#### iter\n",
    "Number of iterations (epochs) over the corpus.\n",
    "\n",
    "### When should you use Word2Vec?\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary.\n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word_model = gensim.models.Word2Vec(subjects+sentences, size=EMBEDDING_DIM, min_count=1, window=5, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each word in the dictionary has an index\n",
    "def word2idx(word, word_model):\n",
    "    if word in word_model.wv.vocab:\n",
    "        return word_model.wv.vocab[word].index\n",
    "    else:\n",
    "        return 1 #default index for non-exsits in vec_model voacb\n",
    "    \n",
    "def idx2word(idx, word_model):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence, word_model):\n",
    "  for w in sentence:\n",
    "      print (\"%d ----> %s\" % (word2idx(w,word_model),w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding the sentences with <end> tag index\n",
    "def sentence_to_indexes(sentence,word_model,max_length_inp):\n",
    "    data_set = [word2idx(w,word_model) for w in sentence]\n",
    "    data_set = tf.keras.preprocessing.sequence.pad_sequences([data_set],max_length_inp, padding='post', value=1)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create matrix of the dataset indexes \n",
    "def create_dataset_word2vec_matrix(sentences,vec_model,max_length_inp):\n",
    "    data_set = []\n",
    "    #creates train data set\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        data_set.append(list())\n",
    "        data_set[i] = ([word2idx(word,vec_model) for word in sentence])\n",
    "    data_set = tf.keras.preprocessing.sequence.pad_sequences(data_set,max_length_inp, padding='post', value=1)\n",
    "    return data_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len, max_subjects_len  = max_length(sentences_train), max_length(subjects_train)\n",
    "x_train = create_dataset_word2vec_matrix(sentences_train,word_model,max_sentence_len)\n",
    "y_train = create_dataset_word2vec_matrix(subjects_train,word_model,max_subjects_len)\n",
    "x_test = sentences_test\n",
    "y_test = subjects_test\n",
    "max_length_inp, max_length_targ  = x_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ----> <start>\n",
      "233 ----> מבחינתי\n",
      "79 ----> אפשר\n",
      "821 ----> להעלות\n",
      "48 ----> אותו\n",
      "4239 ----> לרשימת\n",
      "10519 ----> ההמתנה\n",
      "1130 ----> לשמוע\n",
      "403 ----> הערות\n",
      "14250 ----> ואח\"כ\n",
      "1619 ----> להצבעה\n",
      "1 ----> <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,   233,    79,   821,    48,  4239, 10519,  1130,   403,\n",
       "        14250,  1619,     1,     1,     1]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example sentence\n",
    "convert(example_sentence,word_model)\n",
    "sentence_to_indexes(example_sentence,word_model,max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,   13,    3, ...,    1,    1,    1],\n",
       "        [   0,  240, 1376, ...,    1,    1,    1],\n",
       "        [   0,  151, 5459, ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   0,  910, 4251, ...,    1,    1,    1],\n",
       "        [   0,   18,   12, ...,    1,    1,    1],\n",
       "        [   0,    2,    4, ...,    1,    1,    1]], dtype=int32), (26876, 14))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input matrix\n",
    "x_train, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  import sys\n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "units = 1024\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size = word_model.wv.syn0.shape[0]\n",
    "emdedding_size = word_model.wv.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(y_train))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 14])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, _ = next(iter(dataset))\n",
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the LSTM\n",
    "\n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim,pretrained_weights, maxlen, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[pretrained_weights],input_length=maxlen,trainable=True)\n",
    "    self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units, return_sequences=True, return_state=True))\n",
    "\n",
    "  def call(self, x, forward_h, forward_c, backward_h, backward_c):\n",
    "    x = self.embedding(x)\n",
    "    output, st_forward_h, st_forward_c, st_backward_h, st_backward_c = self.lstm(x, initial_state = [forward_h, forward_c, backward_h, backward_c])\n",
    "    return output, st_forward_h, st_forward_c, st_backward_h, st_backward_c\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 14, 2048)\n",
      "Encoder Hidden forward_h state shape: (batch size, units) (64, 1024)\n",
      "Encoder Hidden forward_c state shape: (batch size, units) (64, 1024)\n",
      "Encoder Hidden backward_h state shape: (batch size, units) (64, 1024)\n",
      "Encoder Hidden backward_c state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, emdedding_size, pretrained_weights, max_length_inp ,units, BATCH_SIZE)\n",
    "forward_h, forward_c, backward_h, backward_c = encoder.initialize_hidden_state()\n",
    "sample_output, forward_h, forward_c, backward_h, backward_c = encoder(example_input_batch, forward_h, forward_c, backward_h, backward_c)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden forward_h state shape: (batch size, units) {}'.format(forward_h.shape))\n",
    "print ('Encoder Hidden forward_c state shape: (batch size, units) {}'.format(forward_c.shape))\n",
    "print ('Encoder Hidden backward_h state shape: (batch size, units) {}'.format(backward_h.shape))\n",
    "print ('Encoder Hidden backward_c state shape: (batch size, units) {}'.format(backward_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 =  tf.keras.layers.Dense(units)\n",
    "    self.W2 =  tf.keras.layers.Dense(units)\n",
    "    self.V =  tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, forward_h, forward_c, backward_h, backward_c, values):\n",
    "    hidden_h =  tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "    hidden_c =  tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "    query =  tf.keras.layers.Concatenate()([hidden_h, hidden_c])\n",
    "\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    context_vector = attention_weights * values\n",
    "\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    \n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, pretrained_weights, maxlen, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[pretrained_weights],input_length=maxlen,trainable=True)\n",
    "    self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True))\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, forward_h, forward_c, backward_h, backward_c, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(forward_h, forward_c, backward_h, backward_c, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the bi-LSTM\n",
    "    output, st_forward_h, st_forward_c, st_backward_h, st_backward_c = self.lstm(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    return x, st_forward_h, st_forward_c, st_backward_h, st_backward_c, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 2048)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 14, 1)\n",
      "Decoder output shape: (batch_size, vocab size) (64, 21000)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(forward_h, forward_c, backward_h, backward_c, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "decoder = Decoder(vocab_size, emdedding_size, pretrained_weights, max_length_targ ,units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _, _, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      forward_h, forward_c, backward_h, backward_c, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, forward_h, forward_c, backward_h, backward_c):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_forward_h, enc_forward_c, enc_backward_h,\\\n",
    "                    enc_backward_c = encoder(inp,forward_h, forward_c, backward_h, backward_c)\n",
    "\n",
    "    dec_forward_h, dec_forward_c, dec_backward_h, dec_backward_c = \\\n",
    "    enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c\n",
    "\n",
    "    dec_input = tf.expand_dims([word_model.wv.vocab['<start>'].index] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_forward_h, dec_forward_c, dec_backward_h,dec_backward_c, _ =\\\n",
    "                decoder(dec_input, dec_forward_h, dec_forward_c, dec_backward_h, dec_backward_c, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7746\n",
      "Epoch 1 Batch 100 Loss 0.6184\n",
      "Epoch 1 Batch 200 Loss 0.8386\n",
      "Epoch 1 Batch 300 Loss 0.7160\n",
      "Epoch 1 Batch 400 Loss 0.6254\n",
      "Epoch 1 Loss 0.7371\n",
      "Time taken for 1 epoch 145.1186227798462 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5638\n",
      "Epoch 2 Batch 100 Loss 0.4822\n",
      "Epoch 2 Batch 200 Loss 0.5849\n",
      "Epoch 2 Batch 300 Loss 0.4553\n",
      "Epoch 2 Batch 400 Loss 0.4175\n",
      "Epoch 2 Loss 0.5142\n",
      "Time taken for 1 epoch 149.01123881340027 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4014\n",
      "Epoch 3 Batch 100 Loss 0.3248\n",
      "Epoch 3 Batch 200 Loss 0.3705\n",
      "Epoch 3 Batch 300 Loss 0.2620\n",
      "Epoch 3 Batch 400 Loss 0.1939\n",
      "Epoch 3 Loss 0.3114\n",
      "Time taken for 1 epoch 144.58724117279053 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2585\n",
      "Epoch 4 Batch 100 Loss 0.1908\n",
      "Epoch 4 Batch 200 Loss 0.2030\n",
      "Epoch 4 Batch 300 Loss 0.1409\n",
      "Epoch 4 Batch 400 Loss 0.1110\n",
      "Epoch 4 Loss 0.1796\n",
      "Time taken for 1 epoch 149.0610339641571 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1738\n",
      "Epoch 5 Batch 100 Loss 0.0925\n",
      "Epoch 5 Batch 200 Loss 0.0865\n",
      "Epoch 5 Batch 300 Loss 0.0784\n",
      "Epoch 5 Batch 400 Loss 0.0653\n",
      "Epoch 5 Loss 0.0987\n",
      "Time taken for 1 epoch 147.13396763801575 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0814\n",
      "Epoch 6 Batch 100 Loss 0.0478\n",
      "Epoch 6 Batch 200 Loss 0.0599\n",
      "Epoch 6 Batch 300 Loss 0.0513\n",
      "Epoch 6 Batch 400 Loss 0.0533\n",
      "Epoch 6 Loss 0.0586\n",
      "Time taken for 1 epoch 150.94043493270874 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0461\n",
      "Epoch 7 Batch 100 Loss 0.0268\n",
      "Epoch 7 Batch 200 Loss 0.0478\n",
      "Epoch 7 Batch 300 Loss 0.0402\n",
      "Epoch 7 Batch 400 Loss 0.0165\n",
      "Epoch 7 Loss 0.0397\n",
      "Time taken for 1 epoch 145.08842539787292 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0241\n",
      "Epoch 8 Batch 100 Loss 0.0345\n",
      "Epoch 8 Batch 200 Loss 0.0380\n",
      "Epoch 8 Batch 300 Loss 0.0356\n",
      "Epoch 8 Batch 400 Loss 0.0170\n",
      "Epoch 8 Loss 0.0283\n",
      "Time taken for 1 epoch 149.1275496482849 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0249\n",
      "Epoch 9 Batch 100 Loss 0.0126\n",
      "Epoch 9 Batch 200 Loss 0.0110\n",
      "Epoch 9 Batch 300 Loss 0.0116\n",
      "Epoch 9 Batch 400 Loss 0.0179\n",
      "Epoch 9 Loss 0.0206\n",
      "Time taken for 1 epoch 144.9997434616089 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0086\n",
      "Epoch 10 Batch 100 Loss 0.0116\n",
      "Epoch 10 Batch 200 Loss 0.0120\n",
      "Epoch 10 Batch 300 Loss 0.0104\n",
      "Epoch 10 Batch 400 Loss 0.0125\n",
      "Epoch 10 Loss 0.0180\n",
      "Time taken for 1 epoch 146.25366353988647 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0092\n",
      "Epoch 11 Batch 100 Loss 0.0256\n",
      "Epoch 11 Batch 200 Loss 0.0082\n",
      "Epoch 11 Batch 300 Loss 0.0086\n",
      "Epoch 11 Batch 400 Loss 0.0157\n",
      "Epoch 11 Loss 0.0179\n",
      "Time taken for 1 epoch 145.25397753715515 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0101\n",
      "Epoch 12 Batch 100 Loss 0.0182\n",
      "Epoch 12 Batch 200 Loss 0.0086\n",
      "Epoch 12 Batch 300 Loss 0.0261\n",
      "Epoch 12 Batch 400 Loss 0.0168\n",
      "Epoch 12 Loss 0.0201\n",
      "Time taken for 1 epoch 146.45145177841187 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0175\n",
      "Epoch 13 Batch 100 Loss 0.0105\n",
      "Epoch 13 Batch 200 Loss 0.0091\n",
      "Epoch 13 Batch 300 Loss 0.0056\n",
      "Epoch 13 Batch 400 Loss 0.0382\n",
      "Epoch 13 Loss 0.0193\n",
      "Time taken for 1 epoch 148.86727595329285 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0247\n",
      "Epoch 14 Batch 100 Loss 0.0287\n",
      "Epoch 14 Batch 200 Loss 0.0178\n",
      "Epoch 14 Batch 300 Loss 0.0191\n",
      "Epoch 14 Batch 400 Loss 0.0215\n",
      "Epoch 14 Loss 0.0179\n",
      "Time taken for 1 epoch 146.20748686790466 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0220\n",
      "Epoch 15 Batch 100 Loss 0.0138\n",
      "Epoch 15 Batch 200 Loss 0.0183\n",
      "Epoch 15 Batch 300 Loss 0.0216\n",
      "Epoch 15 Batch 400 Loss 0.0116\n",
      "Epoch 15 Loss 0.0161\n",
      "Time taken for 1 epoch 144.8815712928772 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0206\n",
      "Epoch 16 Batch 100 Loss 0.0022\n",
      "Epoch 16 Batch 200 Loss 0.0044\n",
      "Epoch 16 Batch 300 Loss 0.0107\n",
      "Epoch 16 Batch 400 Loss 0.0118\n",
      "Epoch 16 Loss 0.0128\n",
      "Time taken for 1 epoch 146.87019872665405 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0123\n",
      "Epoch 17 Batch 100 Loss 0.0051\n",
      "Epoch 17 Batch 200 Loss 0.0141\n",
      "Epoch 17 Batch 300 Loss 0.0047\n",
      "Epoch 17 Batch 400 Loss 0.0138\n",
      "Epoch 17 Loss 0.0124\n",
      "Time taken for 1 epoch 144.56139254570007 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0053\n",
      "Epoch 18 Batch 100 Loss 0.0052\n",
      "Epoch 18 Batch 200 Loss 0.0189\n",
      "Epoch 18 Batch 300 Loss 0.0024\n",
      "Epoch 18 Batch 400 Loss 0.0248\n",
      "Epoch 18 Loss 0.0124\n",
      "Time taken for 1 epoch 146.32097244262695 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0124\n",
      "Epoch 19 Batch 100 Loss 0.0010\n",
      "Epoch 19 Batch 200 Loss 0.0025\n",
      "Epoch 19 Batch 300 Loss 0.0175\n",
      "Epoch 19 Batch 400 Loss 0.0039\n",
      "Epoch 19 Loss 0.0111\n",
      "Time taken for 1 epoch 147.19161438941956 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0066\n",
      "Epoch 20 Batch 100 Loss 0.0027\n",
      "Epoch 20 Batch 200 Loss 0.0085\n",
      "Epoch 20 Batch 300 Loss 0.0305\n",
      "Epoch 20 Batch 400 Loss 0.0011\n",
      "Epoch 20 Loss 0.0095\n",
      "Time taken for 1 epoch 168.67475414276123 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0050\n",
      "Epoch 21 Batch 100 Loss 0.0012\n",
      "Epoch 21 Batch 200 Loss 0.0025\n",
      "Epoch 21 Batch 300 Loss 0.0056\n",
      "Epoch 21 Batch 400 Loss 0.0059\n",
      "Epoch 21 Loss 0.0065\n",
      "Time taken for 1 epoch 153.86040234565735 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0037\n",
      "Epoch 22 Batch 100 Loss 0.0019\n",
      "Epoch 22 Batch 200 Loss 0.0010\n",
      "Epoch 22 Batch 300 Loss 0.0022\n",
      "Epoch 22 Batch 400 Loss 0.0018\n",
      "Epoch 22 Loss 0.0052\n",
      "Time taken for 1 epoch 147.46205258369446 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0083\n",
      "Epoch 23 Batch 100 Loss 0.0005\n",
      "Epoch 23 Batch 200 Loss 0.0015\n",
      "Epoch 23 Batch 300 Loss 0.0045\n",
      "Epoch 23 Batch 400 Loss 0.0010\n",
      "Epoch 23 Loss 0.0043\n",
      "Time taken for 1 epoch 144.18376445770264 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0004\n",
      "Epoch 24 Batch 100 Loss 0.0267\n",
      "Epoch 24 Batch 200 Loss 0.0005\n",
      "Epoch 24 Batch 300 Loss 0.0021\n",
      "Epoch 24 Batch 400 Loss 0.0009\n",
      "Epoch 24 Loss 0.0041\n",
      "Time taken for 1 epoch 146.67894840240479 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0027\n",
      "Epoch 25 Batch 100 Loss 0.0004\n",
      "Epoch 25 Batch 200 Loss 0.0007\n",
      "Epoch 25 Batch 300 Loss 0.0037\n",
      "Epoch 25 Batch 400 Loss 0.0020\n",
      "Epoch 25 Loss 0.0052\n",
      "Time taken for 1 epoch 144.39758920669556 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0112\n",
      "Epoch 26 Batch 100 Loss 0.0058\n",
      "Epoch 26 Batch 200 Loss 0.0302\n",
      "Epoch 26 Batch 300 Loss 0.0029\n",
      "Epoch 26 Batch 400 Loss 0.0291\n",
      "Epoch 26 Loss 0.0063\n",
      "Time taken for 1 epoch 145.6555392742157 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0032\n",
      "Epoch 27 Batch 100 Loss 0.0114\n",
      "Epoch 27 Batch 200 Loss 0.0035\n",
      "Epoch 27 Batch 300 Loss 0.0014\n",
      "Epoch 27 Batch 400 Loss 0.0143\n",
      "Epoch 27 Loss 0.0088\n",
      "Time taken for 1 epoch 144.2722601890564 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0134\n",
      "Epoch 28 Batch 100 Loss 0.0019\n",
      "Epoch 28 Batch 200 Loss 0.0141\n",
      "Epoch 28 Batch 300 Loss 0.0031\n",
      "Epoch 28 Batch 400 Loss 0.0005\n",
      "Epoch 28 Loss 0.0097\n",
      "Time taken for 1 epoch 145.91422986984253 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0033\n",
      "Epoch 29 Batch 100 Loss 0.0009\n",
      "Epoch 29 Batch 200 Loss 0.0037\n",
      "Epoch 29 Batch 300 Loss 0.0004\n",
      "Epoch 29 Batch 400 Loss 0.0068\n",
      "Epoch 29 Loss 0.0071\n",
      "Time taken for 1 epoch 144.46330285072327 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0354\n",
      "Epoch 30 Batch 100 Loss 0.0003\n",
      "Epoch 30 Batch 200 Loss 0.0006\n",
      "Epoch 30 Batch 300 Loss 0.0034\n",
      "Epoch 30 Batch 400 Loss 0.0128\n",
      "Epoch 30 Loss 0.0061\n",
      "Time taken for 1 epoch 146.1199860572815 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    inputs = sentence_to_indexes(sentence,word_model, max_length_inp)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    forward_h  = tf.zeros((1, units))\n",
    "    forward_c  = tf.zeros((1, units))\n",
    "    backward_h = tf.zeros((1, units))\n",
    "    backward_c = tf.zeros((1, units))\n",
    "\n",
    "    enc_out, enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c = encoder(inputs, forward_h, forward_c, backward_h, backward_c)\n",
    "\n",
    "    dec_forward_h, dec_forward_c, dec_backward_h, dec_backward_c = enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c\n",
    "\n",
    "\n",
    "    dec_input = tf.expand_dims([word_model.wv.vocab['<start>'].index], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_forward_h, dec_forward_c, dec_backward_h, dec_backward_c, attention_weights = decoder(dec_input,\n",
    "                                                                                                               dec_forward_h, dec_forward_c, dec_backward_h, dec_backward_c,\n",
    "                                                                                                               enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if word_model.wv.index2word[predicted_id] != '<end>':\n",
    "            result += word_model.wv.index2word[predicted_id] + ' '\n",
    "\n",
    "        if word_model.wv.index2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDTree - nearest-neighbor\n",
    "\n",
    "The algorithm used is described in Maneewongvatana and Mount 1999. The general idea is that the kd-tree is a binary tree, each of whose nodes represents an axis-aligned hyperrectangle. Each node specifies an axis and splits the set of points based on whether their coordinate along that axis is greater than or less than a particular value.\n",
    "\n",
    "During construction, the axis and splitting point are chosen by the “sliding midpoint” rule, which ensures that the cells do not all become long and thin.\n",
    "\n",
    "The tree can be queried for the r closest neighbors of any given point (optionally returning only those within some maximum distance of the point). It can also be queried, with a substantial gain in efficiency, for the r approximate closest neighbors.\n",
    "\n",
    "For large dimensions (20 is already large) do not expect this to run significantly faster than brute force. High-dimensional nearest-neighbor queries are a substantial open problem in computer science.\n",
    "\n",
    "The tree also supports all-neighbors queries, both with arrays of points and with other kd-trees. These do use a reasonably efficient algorithm, but the kd-tree is not necessarily the best data structure for this sort of calculation.\n",
    "\n",
    "![kdtree](img/kdtree-ops.png \"sentences_tagged\")\n",
    "\n",
    "`class scipy.spatial.KDTree(data, leafsize=10)[source]`\n",
    "\n",
    "kd-tree for quick nearest-neighbor lookup\n",
    "\n",
    "This class provides an index into a set of k-dimensional points which can be used to rapidly look up the nearest neighbors of any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KDtree_nearest(test_sentence,result):\n",
    "    A = [word_model[word] for word in test_sentence.split()]\n",
    "    tree = spatial.KDTree(A)\n",
    "    if(len(result)>1):\n",
    "        word_result = result.split()\n",
    "    else:word_result=result\n",
    "    a = word_model[word_result[0].rstrip().strip()]\n",
    "    index = tree.query(a)[1]\n",
    "    return test_sentence.split()[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "* Run **KDTree** to find the right word from the sentence*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    result = KDtree_nearest(sentence,result)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted subject: {}'.format(result))\n",
    "    sentence = sentence[::-1]\n",
    "    result = result[::-1]\n",
    "    sentence_splited = sentence.split(' ')\n",
    "    #to fix the hebrew changes\n",
    "    sentence_splited.reverse()\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence_splited, result.split(' '))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restoring the latest checkpoint in checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f96d4b83eb8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence = \"הוא לא יודע מה עובר עליי\"\n",
    "# result = tag(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8959"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences_test = [listToString(sen[1:-1]) for sen in sentences_test]\n",
    "new_subjects_test = [listToString(sen[1:-1]) for sen in subjects_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tag(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    if len(result)>0:\n",
    "        result = KDtree_nearest(sentence,result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181ea2f7773747db971db1dc0daeec48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8959), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/home/mishka/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "counter = 0\n",
    "for index,sen1 in enumerate(tqdm(new_sentences_test)):\n",
    "    if new_tag(sen1) == new_subjects_test[index]:\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07299921866279718"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter/len(new_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
